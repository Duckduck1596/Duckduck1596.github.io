# 1. 파일과 경로 기본

- 경로: 파일 위치(절대경로/상대경로)
- 루트 폴더
 - Windows: C:\
 - macOS/Linux: /
- raw string: 경로에 \(백슬래시) 많이 쓰면 r"C:\Users\Pentest" 처럼 r"" 권장
``
import os

# 현재 작업 디렉터리
cwd = os.getcwd()
print("CWD:", cwd)

# 파일/디렉터리 판별
print(os.path.isfile("example.txt"))  # True/False
print(os.path.isdir("example_folder"))  # True/False

```
---

# 2. 디렉터리 조회 & 순회

```
import os

# 현재/상위 디렉터리 목록
print(os.listdir('.'))
print(os.listdir('..'))

# 트리 순회 (재귀)
for dirpath, dirnames, filenames in os.walk(r"C:\python_ex"):
    print("DIR:", dirpath)
    print("SUB:", dirnames)
    print("FILES:", filenames)
```

**확장자 필터링 예시** (예: uploads 아래 .txt만)
```
import os

dir_path = "uploads"
txt_files = [f for f in os.listdir(dir_path) if f.endswith(".txt")]
print(txt_files)
```
# 3. 파일 읽기/쓰기 기본기
```
# 쓰기 (자동 close)
with open('example.txt', 'w', encoding='utf-8') as f:
    f.write('Line 1\nLine 2\n')

# read(): 전체
with open('example.txt', 'r', encoding='utf-8') as f:
    print(f.read())

# readline(): 한 줄씩
with open('example.txt', 'r', encoding='utf-8') as f:
    print(f.readline())
    print(f.readline())

# readlines(): 리스트로
with open('example.txt', 'r', encoding='utf-8') as f:
    for line in f.readlines():
        print(line.strip())
```

**디렉터리 내 txt 모두 읽기**
```
import os

dir_path = "uploads"
for name in os.listdir(dir_path):
    if not name.endswith(".txt"):
        continue
    path = os.path.join(dir_path, name)
    with open(path, 'r', encoding='utf-8') as f:
        print(f"[{name}]")
        print(f.read())
        print("-"*40)
```

# 4. 실시간 파일 생성 모니터링(폴링 방식)
```
import os, time
from datetime import datetime

dir_path = "uploads"
prev = set(os.listdir(dir_path))

while True:
    now = datetime.now()
    day = now.strftime("%Y-%m-%d")
    clock = now.strftime("%H:%M:%S")

    curr = set(os.listdir(dir_path))
    created = curr - prev

    for fn in created:
        print("새 파일:", fn)
        with open(f"{day}_탐지_보고서.txt", "a", encoding="utf-8") as log:
            log.write("작성자: 조정원\n")
            log.write("주요 내용: 신규 파일 탐지\n")
            log.write(f"시간: {clock}, 파일: {fn}\n")
            log.write("="*23 + "\n")

    prev = curr
    time.sleep(1)
```

# 5. 정규 표현식으로 민감정보 탐지
- 코드/주석에 섞인 계정/이메일/내부 IP/주민번호 등 탐지 예시 (보안 점검용)

```
import os, re

dir_path = "uploads"
email_re = re.compile(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}\b')
jumin_re = re.compile(r'\b\d{6}\s*-\s*\d{7}\b')  # 공백 허용
ip_re = re.compile(r'\b(?:(?:10|127)\.\d{1,3}\.\d{1,3}\.\d{1,3}|192\.168\.\d{1,3}\.\d{1,3}|172\.(?:1[6-9]|2\d|3[0-1])\.\d{1,3}\.\d{1,3})\b')

for name in os.listdir(dir_path):
    if not name.endswith(".txt"):
        continue
    path = os.path.join(dir_path, name)
    with open(path, 'r', encoding='utf-8') as f:
        for i, line in enumerate(f.readlines(), start=1):
            if line.lstrip().startswith(("#","//","<!--")):  # 주석 라인
                print(f"[주석] {path} {i}라인: {line.strip()}")
            if email_re.search(line):
                print(f"[이메일] {path} {i}라인: {line.strip()}")
            if jumin_re.search(line):
                print(f"[주민번호] {path} {i}라인: {line.strip()}")
            if ip_re.search(line):
                print(f"[내부IP] {path} {i}라인: {line.strip()}")

```
✅ **주의**: 탐지/수집은 합법적 범위 & 개인정보보호법/사규 준수 필수! 실제 데이터는 마스킹/암호화 보관.

***웹 스크래핑 vs 크롤링 한눈 요약***
- 스크래핑: 특정 페이지에서 원하는 데이터만 추출
- 크롤링: 링크 따라 다니며 넓은 범위 수집/색인

# 6. Beautiful Soup 빠른 가이드

설치 & 파서 선택
```
pip install beautifulsoup4
# 고속/튼튼: lxml, 브라우저 수준 복원력: html5lib
pip install lxml html5lib
```

기본 파싱 & 선택
```
from bs4 import BeautifulSoup

html = "<html><body><p class='title'>Hello</p></body></html>"
soup = BeautifulSoup(html, 'html.parser')

print(soup.title)             # <title>...</title> (없으면 None)
print(soup.find('p')['class'])  # ['title']
print(soup.find_all('a'))     # 모든 <a>
print(soup.select('p.title')) # CSS 선택자
print(soup.select_one('#id')) # 첫 요소
```

# 7. Requests 핵심
```
import requests

# GET + 쿼리파라미터
r = requests.get('https://httpbin.org/get', params={'k':'v','k2':['v2','v3']})
print(r.url)      # 인코딩 확인
print(r.text)     # 문자열
print(r.content)  # 바이트(이미지/PDF 등)
print(r.json())   # JSON → 파이썬 dict/list

# 헤더(User-Agent) 지정 (차단 회피/정상 브라우저 흉내)
headers = {'User-Agent':'Mozilla/5.0 ... Chrome/140.0.0.0 Safari/537.36'}
r = requests.get('https://example.com', headers=headers)
```

## 실전 예시 ①: 뉴스 타이틀 스크래핑
```
import requests
from bs4 import BeautifulSoup

url = "https://zdnet.co.kr/"
headers = {'User-Agent':'Mozilla/5.0 ... Chrome/140.0.0.0 Safari/537.36'}

r = requests.get(url, headers=headers)
soup = BeautifulSoup(r.text, 'html.parser')

titles = soup.select("body div.contentWrapper div.left_cont div.news1_box div.news_list div div.assetText a > h4")
for t in titles:
    if t and t.string:
        print(t.string.strip())
````

---

#12. 정규식·스크래핑 치트시트

- \d 숫자, \w 영문+숫자+_ , \s 공백류
- + 1회↑, * 0회↑, ? 0~1회
- ^ 시작, $ 끝, .(개행 제외 임의문자)
- 이메일: \b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}\b
- 내부IP(사설): 10.x.x.x, 172.16~31.x.x, 192.168.x.x
= 주민번호(형식만): \b\d{6}\s*-\s*\d{7}\b
- BeautifulSoup 선택
  - find()/find_all(): 태그/속성/텍스트
  - select()/select_one(): CSS 선택자
- Requests 응답
  - r.text: 문자열
  - r.content: 바이트(이미지/PDF)
  - r.json(): JSON → dict/list







